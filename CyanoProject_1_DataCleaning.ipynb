{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zzm6Aupmoq5k"
      },
      "outputs": [],
      "source": [
        "## Read in csvs\n",
        "import pandas as pd\n",
        "\n",
        "# google trends data (Ohio, LA, SF, Sac)\n",
        "ohio_google = pd.read_csv(\"Ohio_wholestate.csv\", skiprows=2) #we have extra info in first 2 rows, so skipping those\n",
        "caLA_google = pd.read_csv(\"California_LosAngeles.csv\", skiprows=2)\n",
        "caSF_google = pd.read_csv(\"California_SanFrancisco.csv\", skiprows=2)\n",
        "caSac_google = pd.read_csv(\"California_Sacramento.csv\", skiprows=2)\n",
        "cawholestate_google = pd.read_csv(\"California_wholestate.csv\", skiprows = 2)\n",
        "\n",
        "# Lake Erie microcystins\n",
        "df = pd.read_csv('mcyst.csv', encoding='latin1')\n",
        "\n",
        "# California bloom report\n",
        "bloom_reports = pd.read_csv(\"bloom-report.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cleaning Lake Erie microcystins data\n",
        "import numpy as np\n",
        "\n",
        "df.replace({'Particulate Microcystin (µg/L)': {'<0.1': 0, np.nan: 0},\n",
        "            'Dissolved Microcystin (µg/L)': {'<0.1': 0, np.nan: 0}}, inplace=True)\n",
        "#print(df)\n",
        "\n",
        "# Step 1: Change date format to mm/yyyy and fill missing months\n",
        "df['Particulate Microcystin (µg/L)'] = pd.to_numeric(df['Particulate Microcystin (µg/L)'], errors='coerce')\n",
        "df['Dissolved Microcystin (µg/L)'] = pd.to_numeric(df['Dissolved Microcystin (µg/L)'], errors='coerce')\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.set_index('Date').resample('M').sum()\n",
        "df = df.reset_index()\n",
        "df['Date'] = df['Date'].dt.strftime('%m/%Y')\n",
        "\n",
        "#print(df)\n",
        "\n",
        "# Step 4: Create a new column representing YYYYMM\n",
        "df['YYYYMM'] = pd.to_datetime(df['Date'], format='%m/%Y').dt.strftime('%Y%m')\n",
        "\n",
        "# Step 5: Sort the DataFrame by 'YYYYMM' in ascending order\n",
        "df = df.sort_values(by='YYYYMM')\n",
        "\n",
        "# Step 6: Drop the temporary 'YYYYMM' column\n",
        "df.drop(columns=['YYYYMM'], inplace=True)\n",
        "#print(df.head())"
      ],
      "metadata": {
        "id": "lhS0RxMe7vfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cleaning California bloom report data for whole state (1/2)\n",
        "\n",
        "#print(bloom_reports)\n",
        "#print(bloom_reports.shape[0]) #2841 total\n",
        "\n",
        "## (1) removing columns we don't care about\n",
        "\n",
        "# columns we care about\n",
        "columns = [\"Bloom_Date_Created\", \"Water_Body_Name\", \"Regional_Water_Board\", \"Reported_Advisory_Types\", \"Water_Body_Type\",\n",
        "           \"Advisory_Recommended\"]\n",
        "bloom_reports_cleaning = bloom_reports.filter(items = columns)\n",
        "\n",
        "## (2) dealing w/ missing data and making labeling consistent\n",
        "\n",
        "# seeing if there is any missing data\n",
        "print(bloom_reports_cleaning.isnull().sum())\n",
        "# we will decided how to deal with this later but Reported_Adivsory_Type is missing a huge chunk of data\n",
        "# we may decide to use \"Advisory_Recommended\" instead\n",
        "\n",
        "# but for now, let's remove the 143 rows that are missing dates\n",
        "bloom_reports_cleaning.dropna(subset=['Bloom_Date_Created'], inplace=True)\n",
        "#print(bloom_reports_cleaning.shape[0]) # now 2698 total\n",
        "\n",
        "# and for those missing \"Water_Body_Type\", let's see if we can infer that information from the water body name\n",
        "#print(bloom_reports_cleaning[\"Water_Body_Type\"].unique())\n",
        "# we have reservoir, wadeable stream, nonwadeable stream, pond, wetland, stormwater retention, and lake\n",
        "# combine nonwadeable and wadeable stream to \"rivers & streams\" and make reservoirs and ponds as \"Lake\"\n",
        "# and put stormwater retention into other\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Wadeable stream\",  value=\"Rivers & streams\")\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Nonwadeable stream\",  value=\"Rivers & streams\")\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Reservoir\",  value=\"Lake\")\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Pond (<1 ha)\",  value=\"Lake\")\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Stormwater retention\",  value=\"Other\")\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.replace(to_replace= \"Wetland\",  value=\"Other\")\n",
        "\n",
        "# seeing what water_body_names exist\n",
        "#print(bloom_reports_cleaning[\"Water_Body_Name\"].unique())\n",
        "\n",
        "# making function to assign water body type\n",
        "# making new column for marine as there are shorelines, estuaries and lagoons!\n",
        "def assign_water_body_type(name):\n",
        "  if \"lake\" in name.lower():\n",
        "    return \"Lake\"\n",
        "  elif \"reservoir\" in name.lower():\n",
        "    return \"Lake\"\n",
        "  elif \"pond\" in name.lower():\n",
        "    return \"Lake\"\n",
        "  elif \"river\" in name.lower():\n",
        "    return(\"Rivers & streams\")\n",
        "  elif \"stream\" in name.lower():\n",
        "    return(\"Rivers & streams\")\n",
        "  elif \"creek\" in name.lower():\n",
        "    return(\"Rivers & streams\")\n",
        "  elif \"bay\" in name.lower():\n",
        "    return(\"Marine\")\n",
        "  elif \"estuary\" in name.lower():\n",
        "    return(\"Marine\")\n",
        "  elif \"beach\" in name.lower():\n",
        "    return(\"Marine\")\n",
        "  elif \"sea\" in name.lower():\n",
        "    return(\"Marine\")\n",
        "  else:\n",
        "    return(\"Other\")\n",
        "\n",
        "# make a new column for above function\n",
        "bloom_reports_cleaning[\"Derived_Water_Body_Type\"] = bloom_reports_cleaning[\"Water_Body_Name\"].apply(assign_water_body_type)\n",
        "\n",
        "# if we are missing information from the original water_body_type column, use the derived type to fill that in\n",
        "# otherwise keep original information inputted by the state official\n",
        "def adjust_water_body(row):\n",
        "  if pd.isnull(row[\"Water_Body_Type\"]):\n",
        "    return row[\"Derived_Water_Body_Type\"]\n",
        "  else:\n",
        "    return row[\"Water_Body_Type\"]\n",
        "\n",
        "# apply function across rows\n",
        "bloom_reports_cleaning[\"Final_Water_Body_Type\"] = bloom_reports_cleaning.apply(adjust_water_body, axis = 1)\n",
        "#print(bloom_reports_cleaning[\"Final_Water_Body_Type\"].unique())\n",
        "\n",
        "# dealing with advisory level posted or recommended (we want to be \"None\", \"Caution\", \"Warning\", or \"Danger\")\n",
        "#print(bloom_reports_cleaning[\"Reported_Advisory_Types\"].unique())\n",
        "#print(bloom_reports_cleaning[\"Advisory_Recommended\"].unique())\n",
        "# we will work with \"Reported_Advisory_Types\" because in this case we know communication actually happened\n",
        "# a \"general awareness sign\" probably means there was no public health issues regarding the bloom\n",
        "# as a \"caution\" level or higher requires a caution sign\n",
        "\n",
        "# assigning report level with above criteria in mind\n",
        "def assign_report_level(name):\n",
        "  test = str(name) # column not processing as a string\n",
        "  if \"caution\" in test.lower():\n",
        "    return \"Caution\"\n",
        "  elif \"danger\" in test.lower():\n",
        "    return \"Danger\"\n",
        "  elif \"warning\" in test.lower():\n",
        "    return \"Warning\"\n",
        "  elif \"none\" in test.lower():\n",
        "    return \"None\"\n",
        "  elif \"alert sign\" in test.lower():\n",
        "    return \"Caution\"\n",
        "  elif \"general awareness\" in test.lower():\n",
        "    return \"None\"\n",
        "  elif \"NA - refer to Report Details\" in test.lower():\n",
        "    return \"Unknown\"\n",
        "  else:\n",
        "    return \"Unknown\"\n",
        "\n",
        "# apply function\n",
        "bloom_reports_cleaning[\"Adjusted_Reported_Advisory\"] = bloom_reports_cleaning[\"Reported_Advisory_Types\"].apply(assign_report_level)\n",
        "#print(bloom_reports_cleaning[\"Adjusted_Reported_Advisory\"].unique()) # we now have 5 categories- 4 report levels and unknown\n",
        "\n",
        "## (3) aggregate by month and year and calculate number of reports per month\n",
        "\n",
        "# making everything the appropriate data type (string) instead of an object\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.convert_dtypes()\n",
        "\n",
        "# need to make the date column a datetime object rather than a string\n",
        "bloom_reports_cleaning[\"Bloom_Date_Created\"] = pd.to_datetime(bloom_reports_cleaning[\"Bloom_Date_Created\"])\n",
        "\n",
        "# get month and year from datetime object\n",
        "bloom_reports_cleaning[\"Year\"] = bloom_reports_cleaning[\"Bloom_Date_Created\"].dt.year\n",
        "bloom_reports_cleaning[\"Month\"] = bloom_reports_cleaning[\"Bloom_Date_Created\"].dt.month\n",
        "\n",
        "# now, can aggregate by location -> year -> and then month\n",
        "aggregate = bloom_reports_cleaning.groupby(['Year', 'Month']).size()\n",
        "bloom_reports_aggregated = aggregate.reset_index()\n",
        "bloom_reports_aggregated.rename(columns={0: \"Total_Num_Reports\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated.head())\n",
        "#print(bloom_reports_aggregated.shape[0]) # this compacts our dataframe down to 94 rows\n",
        "\n",
        "# now, aggregate by waterbody type -> location -> year -> and then month\n",
        "aggregate_waterbody = bloom_reports_cleaning.groupby([\"Final_Water_Body_Type\", 'Year', 'Month']).size()\n",
        "bloom_reports_aggregated_waterbody = aggregate_waterbody.reset_index()\n",
        "bloom_reports_aggregated_waterbody.rename(columns={0: \"Num_Reports_WB\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated_waterbody.head())\n",
        "\n",
        "# creating mini dataframes for each category (there is probably a better way to do this...)\n",
        "lake_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Lake\"]\n",
        "lake_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Lake\"}, inplace = True)\n",
        "lake_counts = lake_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(lake_counts.head())\n",
        "river_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Rivers & streams\"]\n",
        "river_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Rivers\"}, inplace = True)\n",
        "river_counts = river_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(river_counts.head())\n",
        "marine_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Marine\"]\n",
        "marine_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Marine\"}, inplace = True)\n",
        "marine_counts = marine_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(marine_counts.head())\n",
        "other_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Other\"]\n",
        "other_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Other\"}, inplace = True)\n",
        "other_counts = other_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(other_counts.head())\n",
        "\n",
        "# merge into bloom_reports_aggregated total based on metro, year, month\n",
        "final_ws = pd.merge(bloom_reports_aggregated, lake_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = 'left')\n",
        "final_ws = pd.merge(final_ws, river_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = 'left')\n",
        "final_ws = pd.merge(final_ws, marine_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = 'left')\n",
        "final_ws = pd.merge(final_ws, other_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = 'left')\n",
        "final_ws = final_ws.fillna(0) # fill in missing values as 0\n",
        "#print(final_ws.head()) # looks good!\n",
        "#print(final_ws.shape[0]) # still the same size (94) so everything is good!\n",
        "\n",
        "# now, lastly by advisory type -> location -> year -> and then month\n",
        "aggregated_advisory = bloom_reports_cleaning.groupby([\"Adjusted_Reported_Advisory\", 'Year', 'Month']).size()\n",
        "bloom_reports_aggregated_advisory= aggregated_advisory.reset_index()\n",
        "bloom_reports_aggregated_advisory.rename(columns={0: \"Num_Reports_Advisory\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated_advisory.head())\n",
        "\n",
        "# creating mini dataframes for each category (there is probably a better way to do this...)\n",
        "noadvisory_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"None\"]\n",
        "noadvisory_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_NoAdvisory\"}, inplace = True)\n",
        "noadvisory_counts = noadvisory_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(noadvisory_counts.head())\n",
        "caution_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Caution\"]\n",
        "caution_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Caution\"}, inplace = True)\n",
        "caution_counts = caution_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(caution_counts.head())\n",
        "warning_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Warning\"]\n",
        "warning_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Warning\"}, inplace = True)\n",
        "warning_counts = warning_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(warning_counts.head())\n",
        "danger_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Danger\"]\n",
        "danger_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Danger\"}, inplace = True)\n",
        "danger_counts = danger_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(danger_counts.head())\n",
        "unknown_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Unknown\"]\n",
        "unknown_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_UnknownAdvisory\"}, inplace = True)\n",
        "unknown_counts = unknown_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(unknown_counts.head())\n",
        "\n",
        "# merge into bloom_reports_aggregated total based on metro, year, month\n",
        "final_ws = pd.merge(final_ws, noadvisory_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = \"left\")\n",
        "final_ws = pd.merge(final_ws, caution_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = \"left\")\n",
        "final_ws = pd.merge(final_ws, warning_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = \"left\")\n",
        "final_ws = pd.merge(final_ws, danger_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = \"left\")\n",
        "final_ws = pd.merge(final_ws, unknown_counts, left_on=[\"Year\", \"Month\"], right_on=[\"Year\", \"Month\"], how = \"left\")\n",
        "final_ws = final_ws.fillna(0) # fill in missing values as 0\n",
        "#print(final_ws.tail()) # looks good!\n",
        "#print(final_ws.shape[0]) # still the same size (94) so everything is good!\n",
        "\n",
        "## (5) calculate percentages of each category\n",
        "\n",
        "# simple pandas math!\n",
        "final_ws[\"Percent_Lake\"] = (final_ws[\"Num_Reports_Lake\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_River\"] = (final_ws[\"Num_Reports_Rivers\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_Marine\"] = (final_ws[\"Num_Reports_Marine\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_Other\"] = (final_ws[\"Num_Reports_Other\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_NoAdvisory\"] = (final_ws[\"Num_Reports_NoAdvisory\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_Caution\"] = (final_ws[\"Num_Reports_Caution\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_Warning\"] = (final_ws[\"Num_Reports_Warning\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_Danger\"] = (final_ws[\"Num_Reports_Danger\"] / final_ws[\"Total_Num_Reports\"])\n",
        "final_ws[\"Percent_UnknownAdvisory\"] = (final_ws[\"Num_Reports_UnknownAdvisory\"] / final_ws[\"Total_Num_Reports\"])\n",
        "#print(final_ws.head())"
      ],
      "metadata": {
        "id": "WwibllVTEG5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386ea3a9-2a69-4434-8693-80319111c364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bloom_Date_Created          143\n",
            "Water_Body_Name               0\n",
            "Regional_Water_Board          0\n",
            "Reported_Advisory_Types    1733\n",
            "Water_Body_Type            1100\n",
            "Advisory_Recommended       1387\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-65fa9948732b>:117: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  bloom_reports_cleaning[\"Bloom_Date_Created\"] = pd.to_datetime(bloom_reports_cleaning[\"Bloom_Date_Created\"])\n",
            "<ipython-input-4-65fa9948732b>:138: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  lake_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Lake\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:142: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  river_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Rivers\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:146: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  marine_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Marine\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:150: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  other_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Other\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:171: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  noadvisory_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_NoAdvisory\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:175: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caution_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Caution\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:179: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  warning_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Warning\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:183: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  danger_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Danger\"}, inplace = True)\n",
            "<ipython-input-4-65fa9948732b>:187: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  unknown_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_UnknownAdvisory\"}, inplace = True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cleaning California bloom report dataset for 3 metros (2/2)\n",
        "\n",
        "## (1) filtering out for metros we care about\n",
        "\n",
        "# rows we care about\n",
        "locations = [\"Region 2 - San Francisco Bay\", \"Region 4 - Los Angeles\", \"Region 5 - Central Valley\", \"Region 8 - Santa Ana\"]\n",
        "bloom_reports_cleaning = bloom_reports_cleaning[(bloom_reports_cleaning.Regional_Water_Board == locations[0]) |\n",
        "                                                (bloom_reports_cleaning.Regional_Water_Board == locations[1]) |\n",
        "                                                (bloom_reports_cleaning.Regional_Water_Board == locations[2]) |\n",
        "                                                (bloom_reports_cleaning.Regional_Water_Board == locations[3])]\n",
        "\n",
        "## (2) ID appropriate regions with matching Google Trends metros\n",
        "# Region 2 = \"SF\", Region 4 & 8 = \"LA\", Region 5 = \"Sac\"\n",
        "\n",
        "# making a function to assign metro\n",
        "def assign_metro(region):\n",
        "  if region == locations[0]:\n",
        "    return \"SF\"\n",
        "  elif region == locations[2]:\n",
        "    return \"Sac\"\n",
        "  else:\n",
        "    return \"LA\"\n",
        "\n",
        "# assigning metro to \"Google_Metro\" column\n",
        "bloom_reports_cleaning['Google_Metro'] = bloom_reports_cleaning['Regional_Water_Board'].apply(assign_metro)\n",
        "\n",
        "## (3) aggregate by metro and then by month number of reports per month\n",
        "\n",
        "# making everything the appropriate data type (string) instead of an object\n",
        "bloom_reports_cleaning = bloom_reports_cleaning.convert_dtypes()\n",
        "\n",
        "# need to make the date column a datetime object rather than a string\n",
        "bloom_reports_cleaning[\"Bloom_Date_Created\"] = pd.to_datetime(bloom_reports_cleaning[\"Bloom_Date_Created\"])\n",
        "\n",
        "# get month and year from datetime object\n",
        "bloom_reports_cleaning[\"Year\"] = bloom_reports_cleaning[\"Bloom_Date_Created\"].dt.year\n",
        "bloom_reports_cleaning[\"Month\"] = bloom_reports_cleaning[\"Bloom_Date_Created\"].dt.month\n",
        "\n",
        "# now, can aggregate by location -> year -> and then month\n",
        "aggregate = bloom_reports_cleaning.groupby(['Google_Metro', 'Year', 'Month']).size()\n",
        "bloom_reports_aggregated = aggregate.reset_index()\n",
        "bloom_reports_aggregated.rename(columns={0: \"Total_Num_Reports\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated.head())\n",
        "#print(bloom_reports_aggregated.shape[0]) # this compacts our dataframe down to 200 rows\n",
        "\n",
        "# now, aggregate by waterbody type -> location -> year -> and then month\n",
        "aggregate_waterbody = bloom_reports_cleaning.groupby([\"Final_Water_Body_Type\", 'Google_Metro', 'Year', 'Month']).size()\n",
        "bloom_reports_aggregated_waterbody = aggregate_waterbody.reset_index()\n",
        "bloom_reports_aggregated_waterbody.rename(columns={0: \"Num_Reports_WB\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated_waterbody.head())\n",
        "\n",
        "# creating mini dataframes for each category (there is probably a better way to do this...)\n",
        "lake_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Lake\"]\n",
        "lake_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Lake\"}, inplace = True)\n",
        "lake_counts = lake_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(lake_counts.head())\n",
        "river_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Rivers & streams\"]\n",
        "river_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Rivers\"}, inplace = True)\n",
        "river_counts = river_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(river_counts.head())\n",
        "marine_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Marine\"]\n",
        "marine_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Marine\"}, inplace = True)\n",
        "marine_counts = marine_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(marine_counts.head())\n",
        "other_counts = bloom_reports_aggregated_waterbody.loc[bloom_reports_aggregated_waterbody[\"Final_Water_Body_Type\"] == \"Other\"]\n",
        "other_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Other\"}, inplace = True)\n",
        "other_counts = other_counts.drop(labels = \"Final_Water_Body_Type\", axis = 1)\n",
        "#print(other_counts.head())\n",
        "\n",
        "# merge into bloom_reports_aggregated total based on metro, year, month\n",
        "final_metros = pd.merge(bloom_reports_aggregated, lake_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = 'left')\n",
        "final_metros = pd.merge(final_metros, river_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = 'left')\n",
        "final_metros = pd.merge(final_metros, marine_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = 'left')\n",
        "final_metros = pd.merge(final_metros, wetland_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = 'left')\n",
        "final_metros = pd.merge(final_metros, other_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = 'left')\n",
        "final_metros = final_metros.fillna(0) # fill in missing values as 0\n",
        "#print(final_metros.head()) # looks good!\n",
        "#print(final_metros.shape[0]) # still the same size (200) so everything is good!\n",
        "\n",
        "# now, lastly by advisory type -> location -> year -> and then month\n",
        "aggregated_advisory = bloom_reports_cleaning.groupby([\"Adjusted_Reported_Advisory\", 'Google_Metro', 'Year', 'Month']).size()\n",
        "bloom_reports_aggregated_advisory= aggregated_advisory.reset_index()\n",
        "bloom_reports_aggregated_advisory.rename(columns={0: \"Num_Reports_Advisory\"}, inplace = True) # rename column\n",
        "#print(bloom_reports_aggregated_advisory.head())\n",
        "\n",
        "# creating mini dataframes for each category (there is probably a better way to do this...)\n",
        "noadvisory_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"None\"]\n",
        "noadvisory_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_NoAdvisory\"}, inplace = True)\n",
        "noadvisory_counts = noadvisory_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(noadvisory_counts.head())\n",
        "caution_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Caution\"]\n",
        "caution_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Caution\"}, inplace = True)\n",
        "caution_counts = caution_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(caution_counts.head())\n",
        "warning_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Warning\"]\n",
        "warning_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Warning\"}, inplace = True)\n",
        "warning_counts = warning_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(warning_counts.head())\n",
        "danger_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Danger\"]\n",
        "danger_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Danger\"}, inplace = True)\n",
        "danger_counts = danger_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(danger_counts.head())\n",
        "unknown_counts = bloom_reports_aggregated_advisory.loc[bloom_reports_aggregated_advisory[\"Adjusted_Reported_Advisory\"] == \"Unknown\"]\n",
        "unknown_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_UnknownAdvisory\"}, inplace = True)\n",
        "unknown_counts = unknown_counts.drop(labels = \"Adjusted_Reported_Advisory\", axis = 1)\n",
        "#print(unknown_counts.head())\n",
        "\n",
        "# merge into bloom_reports_aggregated total based on metro, year, month\n",
        "final_metros = pd.merge(final_metros, noadvisory_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = \"left\")\n",
        "final_metros = pd.merge(final_metros, caution_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = \"left\")\n",
        "final_metros = pd.merge(final_metros, warning_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = \"left\")\n",
        "final_metros = pd.merge(final_metros, danger_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = \"left\")\n",
        "final_metros = pd.merge(final_metros, unknown_counts, left_on=[\"Google_Metro\", \"Year\", \"Month\"], right_on=[\"Google_Metro\", \"Year\", \"Month\"], how = \"left\")\n",
        "final_metros = final_metros.fillna(0) # fill in missing values as 0\n",
        "#print(final_metros.tail()) # looks good!\n",
        "#print(final_metros.shape[0]) # still the same size (200) so everything is good!\n",
        "\n",
        "## (5) calculate percentages of each category\n",
        "\n",
        "# simple pandas math!\n",
        "final_metros[\"Percent_Lake\"] = (final_metros[\"Num_Reports_Lake\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_River\"] = (final_metros[\"Num_Reports_Rivers\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_Marine\"] = (final_metros[\"Num_Reports_Marine\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_Other\"] = (final_metros[\"Num_Reports_Other\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_NoAdvisory\"] = (final_metros[\"Num_Reports_NoAdvisory\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_Caution\"] = (final_metros[\"Num_Reports_Caution\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_Warning\"] = (final_metros[\"Num_Reports_Warning\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_Danger\"] = (final_metros[\"Num_Reports_Danger\"] / final_metros[\"Total_Num_Reports\"])\n",
        "final_metros[\"Percent_UnknownAdvisory\"] = (final_metros[\"Num_Reports_UnknownAdvisory\"] / final_metros[\"Total_Num_Reports\"])\n",
        "#print(final.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bsOVPJ9GdQB",
        "outputId": "33249516-81e4-4fbb-fd89-3ec7f2d5bcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-65-b008931c377f>:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  lake_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Lake\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  river_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Rivers\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  marine_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Marine\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:66: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  wetland_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Wetlands\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  other_counts.rename(columns={\"Num_Reports_WB\": \"Num_Reports_Other\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:92: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  noadvisory_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_NoAdvisory\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caution_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Caution\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:100: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  warning_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Warning\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  danger_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_Danger\"}, inplace = True)\n",
            "<ipython-input-65-b008931c377f>:108: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  unknown_counts.rename(columns={\"Num_Reports_Advisory\": \"Num_Reports_UnknownAdvisory\"}, inplace = True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Merging datasets\n",
        "## Match by date (year/month)\n",
        "## Merge by column\n",
        "\n",
        "## Ohio Google trends with microcystins\n",
        "\n",
        "#rename \"Month column to Date\"\n",
        "ohio_google.rename(columns = {'Month': 'Date'}, inplace=True)\n",
        "\n",
        "#changing Month column to mm/yyyy\n",
        "ohio_google['Date'] = pd.to_datetime(ohio_google['Date'])\n",
        "ohio_google['Date'] = ohio_google['Date'].dt.strftime('%m/%Y')\n",
        "#print(ohio_google)\n",
        "\n",
        "#join google trend data with Ohio microcystin data using a left join on Date column\n",
        "mergedOhio = pd.merge(df, ohio_google, how ='left', on ='Date')\n",
        "#print(mergedOhio)\n",
        "mergedOhio.to_csv('OhioFinalData.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ZrqUFGgP8Yc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## California metro Google trends with report data\n",
        "# one for SF, one for LA, one for Sacramento\n",
        "# one for whole state\n",
        "\n",
        "# #SF DATASET\n",
        "#First, let's change the month column to a datetime datatype\n",
        "caSF_google['Month'] = pd.to_datetime(caSF_google['Month'])\n",
        "\n",
        "#We need to pull apart the month and the year into separate columns to match Jordan's data\n",
        "caSF_google['Year'] = caSF_google['Month'].dt.year\n",
        "caSF_google['Month'] = caSF_google['Month'].dt.month\n",
        "\n",
        "#Let's reorganize the order of the columns to it's year, month, blue green algae\n",
        "desired_order = ['Year', 'Month', 'blue green algae: (San Francisco-Oakland-San Jose CA)']\n",
        "\n",
        "# Reassign the DataFrame with the desired column order\n",
        "caSF_google = caSF_google[desired_order]\n",
        "\n",
        "#looks good, now let's add a metro column so we can merge this to Jordan's dataset on the metro column\n",
        "caSF_google['Google_Metro'] = 'SF'\n",
        "\n",
        "#let's also rename our blue green algae column\n",
        "caSF_google.rename(columns={'blue green algae: (San Francisco-Oakland-San Jose CA)': 'blue green algae'}, inplace=True)\n",
        "\n",
        "#Let's go ahead and do the same thing for the LA dataset\n",
        "# we need to merge on year, then month, then county\n",
        "#First, let's change the month column to a datetime datatype\n",
        "caLA_google['Month'] = pd.to_datetime(caLA_google['Month'])\n",
        "\n",
        "#We need to pull apart the month and the year into separate columns to match Jordan's data\n",
        "caLA_google['Year'] = caLA_google['Month'].dt.year\n",
        "caLA_google['Month'] = caLA_google['Month'].dt.month\n",
        "\n",
        "#Let's reorganize the order of the columns to it's year, month, blue green algae\n",
        "desired_order = ['Year', 'Month', 'blue green algae: (Los Angeles CA)']\n",
        "\n",
        "# Reassign the DataFrame with the desired column order\n",
        "caLA_google = caLA_google[desired_order]\n",
        "\n",
        "#looks good, now let's add a metro column so we can merge this to Jordan's dataset on the metro column\n",
        "caLA_google['Google_Metro'] = 'LA'\n",
        "\n",
        "#let's also rename our blue green algae column\n",
        "caLA_google.rename(columns={'blue green algae: (Los Angeles CA)': 'blue green algae'}, inplace=True)\n",
        "\n",
        "# Let's do the same thing for Sac\n",
        "# we need to merge on year, then month, then county\n",
        "#First, let's change the month column to a datetime datatype\n",
        "caSac_google['Month'] = pd.to_datetime(caSac_google['Month'])\n",
        "\n",
        "#We need to pull apart the month and the year into separate columns to match Jordan's data\n",
        "caSac_google['Year'] = caSac_google['Month'].dt.year\n",
        "caSac_google['Month'] = caSac_google['Month'].dt.month\n",
        "\n",
        "#Let's reorganize the order of the columns to it's year, month, blue green algae\n",
        "desired_order = ['Year', 'Month', 'blue green algae: (Sacramento-Stockton-Modesto CA)']\n",
        "\n",
        "# Reassign the DataFrame with the desired column order\n",
        "caSac_google = caSac_google[desired_order]\n",
        "\n",
        "#looks good, now let's add a metro column so we can merge this to Jordan's dataset on the metro column\n",
        "caSac_google['Google_Metro'] = 'Sac'\n",
        "\n",
        "#let's also rename our blue green algae column\n",
        "caSac_google.rename(columns={'blue green algae: (Sacramento-Stockton-Modesto CA)': 'blue green algae'}, inplace=True)\n",
        "\n",
        "#now, let's merge the data\n",
        "#first we will concat all of the metro data into one df to make it easier to merge to Jordan's data\n",
        "mergedMetros = pd.concat([caSac_google, caSF_google], axis=0)\n",
        "mergedMetros = pd.concat([mergedMetros, caLA_google], axis=0)\n",
        "\n",
        "#now let's merge that into the CA df\n",
        "mergedCA_metros = pd.merge(final_metros, mergedMetros, on=['Google_Metro', 'Year', 'Month'], how='left')\n",
        "\n",
        "#write out that data into a csv for download\n",
        "mergedCA_metros.to_csv('CAFinalData_Metros.csv', index=False)\n",
        "\n",
        "# #WHOLE STATE DATASET\n",
        "\n",
        "#First, let's change the month column to a datetime datatype\n",
        "cawholestate_google['Month'] = pd.to_datetime(cawholestate_google['Month'])\n",
        "\n",
        "#We need to pull apart the month and the year into separate columns to match Jordan's data\n",
        "cawholestate_google['Year'] = cawholestate_google['Month'].dt.year\n",
        "cawholestate_google['Month'] = cawholestate_google['Month'].dt.month\n",
        "\n",
        "#Let's reorganize the order of the columns to it's year, month, blue green algae\n",
        "desired_order = ['Year', 'Month', 'blue green algae: (California)']\n",
        "\n",
        "# Reassign the DataFrame with the desired column order\n",
        "cawholestate_google = cawholestate_google[desired_order]\n",
        "\n",
        "#let's also rename our blue green algae column\n",
        "cawholestate_google.rename(columns={'blue green algae: (California)': 'blue green algae'}, inplace=True)\n",
        "\n",
        "# Merge whole state datasets\n",
        "mergedCA_wholestate = pd.merge(final_ws, cawholestate_google, on=['Year', 'Month'], how='left')\n",
        "\n",
        "#write out that data into a csv for download\n",
        "mergedCA_wholestate.to_csv('CAFinalData_WholeState.csv', index=False)"
      ],
      "metadata": {
        "id": "nEGRK8Hww2g8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f27afd2d-f778-4339-e57a-572cf3d51592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-1c763dd17644>:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caSF_google['Google_Metro'] = 'SF'\n",
            "<ipython-input-66-1c763dd17644>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caSF_google.rename(columns={'blue green algae: (San Francisco-Oakland-San Jose CA)': 'blue green algae'}, inplace=True)\n",
            "<ipython-input-66-1c763dd17644>:41: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caLA_google['Google_Metro'] = 'LA'\n",
            "<ipython-input-66-1c763dd17644>:44: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caLA_google.rename(columns={'blue green algae: (Los Angeles CA)': 'blue green algae'}, inplace=True)\n",
            "<ipython-input-66-1c763dd17644>:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caSac_google['Google_Metro'] = 'Sac'\n",
            "<ipython-input-66-1c763dd17644>:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  caSac_google.rename(columns={'blue green algae: (Sacramento-Stockton-Modesto CA)': 'blue green algae'}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #WHOLE STATE DATASET\n",
        "\n",
        "#First, let's change the month column to a datetime datatype\n",
        "cawholestate_google['Month'] = pd.to_datetime(cawholestate_google['Month'])\n",
        "\n",
        "#We need to pull apart the month and the year into separate columns to match Jordan's data\n",
        "cawholestate_google['Year'] = cawholestate_google['Month'].dt.year\n",
        "cawholestate_google['Month'] = cawholestate_google['Month'].dt.month\n",
        "\n",
        "#Let's reorganize the order of the columns to it's year, month, blue green algae\n",
        "desired_order = ['Year', 'Month', 'blue green algae: (California)']\n",
        "\n",
        "# Reassign the DataFrame with the desired column order\n",
        "cawholestate_google = cawholestate_google[desired_order]\n",
        "\n",
        "#let's also rename our blue green algae column\n",
        "cawholestate_google.rename(columns={'blue green algae: (California)': 'blue green algae'}, inplace=True)\n",
        "\n",
        "# Merge whole state datasets\n",
        "mergedCA_wholestate = pd.merge(final_ws, cawholestate_google, on=['Year', 'Month'], how='left')\n",
        "\n",
        "#write out that data into a csv for download\n",
        "mergedCA_wholestate.to_csv('CAFinalData_WholeState.csv', index=False)"
      ],
      "metadata": {
        "id": "cVi2RdpCI4Pg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}